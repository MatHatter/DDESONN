Feature idea: Single-run per-epoch performance tracking

Purpose: Track how performance metrics (e.g., MSE, accuracy, precision) change over epochs for a single run.

Implementation:

Inside train() → inside the epoch loop → after computing validation predictions, record metrics into a single_run_per_epoch_metrics list or data frame.

Store both training and validation metrics if available.

Usage:

Diagnostic only — shows the learning curve for one model.

Useful for detecting overfitting, plateaus, or instability.

Not used for ensemble summaries.

Plotting:

Line plot (metric vs epoch) with separate color/style for training vs validation.

Save to plots/single_run_per_epoch/ folder.

Scope note:

Keep this logic separate from process_performance() so that per-epoch curves are not mixed into the high_mean_df / low_mean_df plots.

process_performance() should remain focused on final aggregated metrics for clean high/low mean grouping.

Future expansion:

Allow specifying which metrics to track per epoch.

Optional moving average smoothing for noisy metrics.

